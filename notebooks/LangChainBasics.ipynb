{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Playground ⛓️🦜\n",
    "The following are based on LangChain's Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = dotenv_values('../.env')['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Components\n",
    "### Schema\n",
    "The basic data types and schemas that are used throughout the codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text\n",
    "Strings are used to interact with language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "my_text = \"What time is it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Messages\n",
    "Some models uses a chat interface. Similar to text but with specified type(System, Human, AI)\n",
    " - **System** - A helpful background context that tell th AI what to do\n",
    " - **Human** - Represents the message coming from a human\n",
    " - **AI** - Message that shows what the AI responded with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For steamed fish, you'll want a wine that complements the delicate flavors of the dish without overpowering it. A few options that pair well with steamed fish are:\\n\\n1. Sauvignon Blanc: This white wine has bright acidity and herbal notes that pair nicely with the lightness of the fish.\\n\\n2. Chardonnay: Look for an unoaked or lightly oaked Chardonnay, as it will enhance the flavors of the fish without overwhelming it.\\n\\n3. Pinot Grigio: A crisp and refreshing white wine with citrus notes that can complement the subtle flavors of the fish.\\n\\n4. Riesling: If you prefer a slightly sweeter wine, a semi-dry or off-dry Riesling can be an excellent choice, as its acidity can balance the flavors of the dish.\\n\\nRemember, personal taste preferences vary, so feel free to try different options to find the one that suits your palate best.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "  [\n",
    "    SystemMessage(content=\"You are a nice AI that helps a user figure out the wine that matches their food.\"),\n",
    "    HumanMessage(content=\"I'm eating steamed fish, what should I drink?\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass more chat history w/ responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"When you're in Akihabara, you can immerse yourself in the world of anime and manga. Here are some things you can do:\\n\\n1. Visit Anime and Manga Stores: Explore the numerous anime and manga shops in Akihabara, such as Mandarake, Animate, and Gamers. You'll find a wide range of merchandise, including DVDs, manga, figurines, and collectibles.\\n\\n2. Maid Cafes: Experience the unique concept of maid cafes, where waitresses dress up as maids and provide entertainment and food. It's a popular subculture in Akihabara, and you can enjoy a fun and interactive dining experience.\\n\\n3. Themed Cafes: Explore the various themed cafes in the area, such as the Gundam Cafe, where you can enjoy food and drinks inspired by the popular Gundam series. There are also cafes themed around popular anime and manga series like Pokemon, Sailor Moon, and more.\\n\\n4. Game Centers: Have fun at the arcades and game centers in Akihabara. You can try your hand at the latest anime-themed arcade games, crane machines to win cute plushies, or even play competitive games with locals.\\n\\n5. Cosplay and Costume Shops: If you're a fan of cosplay, Akihabara is the perfect place to find costumes, wigs, and accessories. You can even participate in cosplay events and attend anime-themed parties.\\n\\n6. Attend Anime Events: Check out if there are any anime events or exhibitions happening during your visit. Akihabara often hosts special events, conventions, and exhibitions where you can meet voice actors, see exclusive previews, and participate in interactive activities.\\n\\nRemember to check the schedules and availability of specific events and activities before you go, as they may vary throughout the year. Enjoy your anime-filled adventure in Akihabara!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "  [\n",
    "    SystemMessage(content=\"You are a nice AI that helps a user figure out where to travel and what to do there.\"),\n",
    "    HumanMessage(content=\"I like anime where should I go?\"),\n",
    "    AIMessage(content=\"You should go to Akihabara, Japan\"),\n",
    "    HumanMessage(content=\"What can I do when I'm there?\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents\n",
    "An object that holds a piece of text and metadata(more info about the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. It contains text from LangChain Documentation.', metadata={'my_document_id': 1234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It contains text from LangChain Documentation.\",\n",
    "        metadata={\n",
    "          'my_document_id': 1234,\n",
    "          'my_document_source': 'The LangChain Papers',\n",
    "          'my_document_create_time': 1680013019\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "The following are the different types of models that are used in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Language Models\n",
    "A model that does text in and text out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe date today is March 1st.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow become a successful businessman? Because he was outstanding in his field! Good luck finding a job, though.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user asks.\"),\n",
    "        HumanMessage(content=\"I would like to get a job, how should I do it?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Embedding Model\n",
    "These models takes text as input and returns a list of floats that hold the semantic meaning of your text.\n",
    ">_*Semantic*_ means 'relating to meaning in language or logic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It's time for lunch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding length: 1536\n",
      "Here is a sample: [0.011875979983118259, -0.006945648918972976, -0.00224773895683153, 0.003370858836966017, -0.008643074238262946]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f'Your embedding length: {len(text_embedding)}')\n",
    "print(f'Here is a sample: {text_embedding[:5]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "#### Prompt Value\n",
    "Refers to the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe statement is incorrect; tomorrow is Tuesday, not Wednesday.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday,\n",
    "\n",
    "What is wrong with the statement?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template\n",
    "An object that helps in creating a PromptValue. A combination of user input, non-static information and a fixed template string.\n",
    "> Like f-string in python for prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: \n",
      "I realy want to travel to Japan, What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "--------------\n",
      "LLM output: \n",
      "Explore the stunning natural scenery and unique culture of Japan.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Notice \"location\" is placeholder for another value later\n",
    "template = \"\"\"\n",
    "I realy want to travel to {location}, What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Japan\")\n",
    "\n",
    "print(f\"Final prompt: {final_prompt}\")\n",
    "print(\"--------------\")\n",
    "print(f\"LLM output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Selectors\n",
    "An easy way to select a series of examples that allow you to dynamically place in-context information into your prompt.\n",
    "Often used when your task is nuanced(meticulous) or you have a large list of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    #This is the embedding class used to produce embeddings which are used to measure semantic\n",
    "    OpenAIEmbeddings(),\n",
    "    #This is the VectorStore class that is used to store the embeddings and do a similarity check\n",
    "    FAISS,\n",
    "    #Number of examples to produce\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will hep select examples\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    # Customization that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Example Input: bird\n",
      "Example Output: nest\n",
      "\n",
      "Input: flower\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "#Select a noun\n",
    "my_noun ='flower'\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' garden'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "A helpful way to format the output of a model. Usually used for structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main concepts:\n",
    "1. Format Instructions - A autogenerated prompt that tels the LLM how to format it's response based off your desired result\n",
    "2. Parser - A method which will extract your model's text output into a desired structure(usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you would like your response structured\n",
    "response_schemas = [\n",
    "    ResponseSchema(name='bad_string', description=\"This is a poorly formatted user input string\"),\n",
    "    ResponseSchema(name='good_string', description='This is your response, a reformatted response')\n",
    "]\n",
    "\n",
    "#How would you like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template your created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted traing from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT:\n",
      "welcom to californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted traing from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input='welcom to californya!')\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to californya!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note there is a possiblity that the out is not properly formatted JSON\n",
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to californya!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateGpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
